{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def basic_sigmoid(x):\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(x+3) #broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习1**：使用numpy实现sigmoid函数。\n",
    "说明：x可以是实数，向量或矩阵。 我们在numpy中使用的表示向量、矩阵等的数据结构称为numpy数组。现阶段你只需了解这些就已足够。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.37754067, 0.5       , 0.62245933, 0.73105858])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([-1,-0.5,0,0.5,1])\n",
    "sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 1\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习2**： 创建函数sigmoid_grad（）计算sigmoid函数相对于其输入x的梯度。公式为：s=a*(1-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进行计算\n"
     ]
    }
   ],
   "source": [
    "print(\"进行计算\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量x的sigmod:[0.26894142 0.37754067 0.5        0.62245933 0.73105858]\n",
      "向量x的梯度:[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "print(\"向量x的sigmod:\"+str(sigmoid(y)))\n",
    "      \n",
    "print(\"向量x的梯度:\"+str(sigmoid_derivative(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改数组形状\n",
    "例如，在计算机科学中，图像由shape为(length,height,depth=3)的3D数组表示。但是，当你读取图像作为算法的输入时，会将其转换为维度为(lenght*height*3,1)的向量。换句话说，将3D阵列“展开”或重塑为1D向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习3**：\n",
    "实现image2vector() ,该输入采用维度为(length, height, 3)的输入，并返回维度为(length*height*3, 1)的向量。例如，如果你想将形为（a，b，c）的数组v重塑为维度为(a*b, 3)的向量，则可以执行以下操作：\n",
    "\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    shape  = image.shape\n",
    "    v = image.reshape(shape[0]*shape[1]*shape[2],1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n",
      "res:[[0.67826139 0.29380381 0.90714982 0.52835647 0.4215251  0.45017551\n",
      "  0.92814219 0.96677647 0.85304703 0.52351845 0.19981397 0.27417313\n",
      "  0.60659855 0.00533165 0.10820313 0.49978937 0.34144279 0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array(\n",
    "    [\n",
    "        #第一层\n",
    "        [\n",
    "            #第一行\n",
    "            [ 0.67826139,  0.29380381],\n",
    "            #第二行\n",
    "            [ 0.90714982,  0.52835647],\n",
    "            #第三行\n",
    "            [ 0.4215251 ,  0.45017551]\n",
    "        ],\n",
    "        #第二层\n",
    "       [\n",
    "           [ 0.92814219,  0.96677647],\n",
    "            [ 0.85304703,  0.52351845],\n",
    "            [ 0.19981397,  0.27417313]\n",
    "       ],\n",
    "        #第三层\n",
    "       [\n",
    "        [ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]\n",
    "       ]\n",
    "    ]\n",
    ")\n",
    "#转换成列向量\n",
    "res = image2vector(image)\n",
    "print (\"image2vector(image) = \" + str(res)) # n*1\n",
    "print(\"res:\"+str(res.T)) # 1*n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行标准化\n",
    "我们在机器学习和深度学习中使用的另一种常见技术是对数据进行标准化。 由于归一化后梯度下降的收敛速度更快，通常会表现出更好的效果。 通过归一化，也就是将x更改为（将x的每个行向量除以其范数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "执行 normalizeRows（）来标准化矩阵的行。 将此函数应用于输入矩阵x之后，x的每一行应为单位长度（即长度为1）向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    x = x/norm\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res:[[0.26726124 0.53452248 0.80178373]\n",
      " [0.         0.8        0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    "    [1,2,3],\n",
    "    [0,4,3]\n",
    "]\n",
    "print(\"res:\"+str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意：\n",
    "在normalizeRows（）中，你可以尝试print查看 x_norm和x的维度，然后重新运行练习cell。 你会发现它们具有不同的w维度。 鉴于x_norm采用x的每一行的范数，这是正常的。 因此，x_norm具有相同的行数，但只有1列。 那么，**当你将x除以x_norm时，它是如何工作的？** 这就是所谓的广播broadcasting，我们现在将讨论它！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm.shape:(2, 1)\n",
      "x.shape:(2, 3)\n",
      "res2:[[0.26726124 0.53452248 0.80178373]\n",
      " [0.         0.8        0.6       ]]\n",
      "#换下轴试一下\n",
      "norm.shape:(1, 3)\n",
      "x.shape:(2, 3)\n",
      "res3:[[1.         0.4472136  0.70710678]\n",
      " [0.         0.89442719 0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows2(x):\n",
    "    # 求横轴范数\n",
    "    norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    x = x/norm\n",
    "    print(\"norm.shape:\"+str(norm.shape))\n",
    "    print(\"x.shape:\"+str(x.shape))\n",
    "    return x\n",
    "x = [\n",
    "    [1,2,3],\n",
    "    [0,4,3]\n",
    "]\n",
    "print(\"res2:\"+str(normalizeRows2(x)))\n",
    "\n",
    "print(\"#换下轴试一下\")\n",
    "def normalizeRows3(x):\n",
    "    # 求纵轴范数\n",
    "    norm = np.linalg.norm(x,axis=0,keepdims=True)\n",
    "    x = x/norm\n",
    "    print(\"norm.shape:\"+str(norm.shape))\n",
    "    print(\"x.shape:\"+str(x.shape))\n",
    "    return x\n",
    "print(\"res3:\"+str(normalizeRows3(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播和softmax函数\n",
    "在numpy中要理解的一个非常重要的概念是“广播”。 这对于在不同形状的数组之间执行数学运算非常有用。 有关广播的完整详细信息，你可以阅读官方的broadcasting documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习:\n",
    "使用numpy实现softmax函数。 你可以将softmax理解为算法需要对两个或多个类进行分类时使用的标准化函数。 你将在本专业的第二门课中了解有关softmax的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]\n",
      " [0.26762315 0.72747516 0.00490169]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp,axis=1,keepdims=True)\n",
    "    s = x_exp/x_sum\n",
    "    return s\n",
    "xs = [\n",
    "    [1,2,3],\n",
    "    [4,5,0]\n",
    "]\n",
    "xs_softmax = softmax(xs)\n",
    "print(str(xs_softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化\n",
    "在深度学习中，通常需要处理非常大的数据集。 因此，非计算最佳函数可能会成为算法中的巨大瓶颈，并可能使模型运行一段时间。 为了确保代码的高效计算，我们将使用向量化。 例如，尝试区分点/外部/元素乘积之间的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.10406299999998758ms\n",
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.24555199999998223ms\n",
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.19180399999996212ms\n",
      "gdot = [27.98533902 24.95128314 19.9442907 ]\n",
      " ----- Computation time = 0.2076939999999805ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.13271900000000336ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.14455400000001895ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- Computation time = 0.1147249999999822ms\n",
      "gdot = [27.98533902 24.95128314 19.9442907 ]\n",
      " ----- Computation time = 0.0950690000001142ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现L1和L2损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1:1.1\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat,y):\n",
    "    loss = np.sum(np.abs(yhat-y))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L1:\"+str(L1(yhat,y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2:0.43\n"
     ]
    }
   ],
   "source": [
    "def L2(yhat,y):\n",
    "    return np.dot((y-yhat),(y-yhat).T)\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2:\"+str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
